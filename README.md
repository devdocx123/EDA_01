# ğŸ§  IPL Auction EDA Journey â€“ Machine Learning Exploration ğŸ

Welcome to my **Exploratory Data Analysis (EDA)** journey as part of my **Machine Learning learning series**!  
Each day, I explore different datasets to strengthen my data understanding, feature engineering, and analytical thinking.

---

## ğŸ“… Day 1 â€“ Data Cleaning & Exploration (IPL Auction Dataset)

**Objective:**  
Kickstart the EDA process by exploring and cleaning the **IPL Auction dataset**, understanding player distribution, and preparing the data for analysis.

**Key Steps:**
- Loaded and explored the raw dataset.
- Handled missing and inconsistent values.
- Detected and removed outliers.
- Performed basic feature engineering.
- Visualized trends like retained, sold, and unsold players using **Matplotlib** and **Seaborn**.

**Files:**
- `Day01/EDA01.ipynb`
- `Day01/iplauction2023_raw-data.csv`
- `Day01/iplauction2023_cleaned-data.csv`

**Outcome:**  
Prepared a clean dataset highlighting player trends and financial insights for further analysis.

---

## âš™ï¸ Day 2 â€“ Feature Engineering on Water Potability Dataset ğŸ’§

**Objective:**  
Apply **Feature Engineering** and **Data Transformation** techniques on a new dataset â€” *Water Potability* â€” to practice handling numerical data and improving data quality for model training.

**Key Steps:**
- Explored relationships between chemical properties (pH, hardness, solids, etc.).
- Handled missing values and treated outliers.
- Scaled and normalized features to bring uniformity.
- Conducted correlation analysis using heatmaps.
- Prepared the dataset for classification model readiness.

**Files:**
- `Day02/EDA02.ipynb`
- `Day02/water_potability_ds2.csv` ( raw dataset)
- `Day02/cleaned_dataset_Water-Potability.csv` (cleaned dataset)

**Outcome:**  
Achieved a balanced, preprocessed dataset with key insights into water quality indicators â€” ready for ML model implementation.

---

ğŸ’§ ğ——ğ—®ğ˜† ğŸ¯ â€“ ğ—™ğ—²ğ—®ğ˜ğ˜‚ğ—¿ğ—² ğ—¦ğ—°ğ—®ğ—¹ğ—¶ğ—»ğ—´ & ğ—–ğ—¼ğ—¿ğ—¿ğ—²ğ—¹ğ—®ğ˜ğ—¶ğ—¼ğ—» ğ—”ğ—»ğ—®ğ—¹ğ˜†ğ˜€ğ—¶ğ˜€
ğ—¢ğ—¯ğ—·ğ—²ğ—°ğ˜ğ—¶ğ˜ƒğ—²

Prepare the Water Potability Dataset for modeling by applying feature scaling and performing correlation analysis to uncover relationships between variables.

ğ—ğ—²ğ˜† ğ—¦ğ˜ğ—²ğ—½ğ˜€

Applied ğ—¦ğ˜ğ—®ğ—»ğ—±ğ—®ğ—¿ğ—±ğ—¦ğ—°ğ—®ğ—¹ğ—²ğ—¿ to normalize numerical features.

Analyzed ğ—°ğ—¼ğ—¿ğ—¿ğ—²ğ—¹ğ—®ğ˜ğ—¶ğ—¼ğ—»ğ˜€ among key parameters like pH, hardness, and solids.

Created ğ—µğ—²ğ—®ğ˜ğ—ºğ—®ğ—½ğ˜€ and ğ—½ğ—®ğ—¶ğ—¿ğ—½ğ—¹ğ—¼ğ˜ğ˜€ to visualize relationships and detect key patterns.

ğ—¢ğ˜‚ğ˜ğ—°ğ—¼ğ—ºğ—²

Established a ğ—°ğ—¼ğ—»ğ˜€ğ—¶ğ˜€ğ˜ğ—²ğ—»ğ˜ ğ—³ğ—²ğ—®ğ˜ğ˜‚ğ—¿ğ—² ğ˜€ğ—°ğ—®ğ—¹ğ—² across variables.

Identified ğ—¸ğ—²ğ˜† ğ—°ğ—¼ğ—¿ğ—¿ğ—²ğ—¹ğ—®ğ˜ğ—²ğ—± ğ—³ğ—²ğ—®ğ˜ğ˜‚ğ—¿ğ—²ğ˜€ influencing water potability.

Prepared data for ğ—³ğ—²ğ—®ğ˜ğ˜‚ğ—¿ğ—² ğ˜€ğ—²ğ—¹ğ—²ğ—°ğ˜ğ—¶ğ—¼ğ—» and ğ—ºğ—¼ğ—±ğ—²ğ—¹ ğ—¯ğ˜‚ğ—¶ğ—¹ğ—±ğ—¶ğ—»ğ—´.

ğŸ“˜ Notebook: - `Day02/EDA02.ipynb`
- `Day02/water_potability_ds2.csv` ( raw dataset)
- `Day02/cleaned_dataset_Water-Potability.csv` (cleaned dataset)

- ---

ğŸ¤– ğ——ğ—®ğ˜† ğŸ° â€“ ğ—–ğ—¼ğ—ºğ—½ğ—®ğ—¿ğ—¶ğ—»ğ—´ ğ—™ğ—²ğ—®ğ˜ğ˜‚ğ—¿ğ—² ğ—¦ğ—°ğ—®ğ—¹ğ—²ğ—¿ğ˜€
ğ—¢ğ—¯ğ—·ğ—²ğ—°ğ˜ğ—¶ğ˜ƒğ—²

Understand how different scaling methods â€” StandardScaler, MinMaxScaler, and RobustScaler â€” affect data transformation and model readiness.

ğ—ğ—²ğ˜† ğ—¦ğ˜ğ—²ğ—½ğ˜€

Compared ğ—¦ğ˜ğ—®ğ—»ğ—±ğ—®ğ—¿ğ—±ğ—¦ğ—°ğ—®ğ—¹ğ—²ğ—¿, ğ— ğ—¶ğ—»ğ— ğ—®ğ˜…ğ—¦ğ—°ğ—®ğ—¹ğ—²ğ—¿, and ğ—¥ğ—¼ğ—¯ğ˜‚ğ˜€ğ˜ğ—¦ğ—°ğ—®ğ—¹ğ—²ğ—¿ on the same dataset.

Visualized ğ—µğ—¼ğ˜„ ğ—²ğ—®ğ—°ğ—µ ğ˜€ğ—°ğ—®ğ—¹ğ—²ğ—¿ ğ˜ğ—¿ğ—®ğ—»ğ˜€ğ—³ğ—¼ğ—¿ğ—ºğ˜€ ğ—±ğ—®ğ˜ğ—® differently.

Evaluated ğ˜„ğ—µğ—¶ğ—°ğ—µ ğ˜€ğ—°ğ—®ğ—¹ğ—²ğ—¿ works best depending on data distribution and outliers.

ğ—¢ğ˜‚ğ˜ğ—°ğ—¼ğ—ºğ—²

âš–ï¸ ğ—¦ğ˜ğ—®ğ—»ğ—±ğ—®ğ—¿ğ—±ğ—¦ğ—°ğ—®ğ—¹ğ—²ğ—¿: Ideal for ğ—»ğ—¼ğ—¿ğ—ºğ—®ğ—¹ğ—¹ğ˜† ğ—±ğ—¶ğ˜€ğ˜ğ—¿ğ—¶ğ—¯ğ˜‚ğ˜ğ—²ğ—± ğ—±ğ—®ğ˜ğ—® (centers around 0, scales by std).

ğŸ“‰ ğ— ğ—¶ğ—»ğ— ğ—®ğ˜…ğ—¦ğ—°ğ—®ğ—¹ğ—²ğ—¿: Perfect for ğ—»ğ—²ğ˜‚ğ—¿ğ—®ğ—¹ ğ—»ğ—²ğ˜ğ˜„ğ—¼ğ—¿ğ—¸ğ˜€ and ğ—¯ğ—¼ğ˜‚ğ—»ğ—±ğ—²ğ—± ğ—¶ğ—»ğ—½ğ˜‚ğ˜ğ˜€ (ğŸ¬â€“ğŸ­).

ğŸ§± ğ—¥ğ—¼ğ—¯ğ˜‚ğ˜€ğ˜ğ—¦ğ—°ğ—®ğ—¹ğ—²ğ—¿: Best for ğ—¼ğ˜‚ğ˜ğ—¹ğ—¶ğ—²ğ—¿-ğ—µğ—²ğ—®ğ˜ƒğ˜† ğ—±ğ—®ğ˜ğ—®, uses ğ—ºğ—²ğ—±ğ—¶ğ—®ğ—» and ğ—œğ—¤ğ—¥.

ğŸ“˜ Notebook: Day04/EDA02.ipynb

- ----

ğ——ğ—®ğ˜† ğŸ± â€” ğ—§ğ—¿ğ—®ğ—¶ğ—»-ğ—§ğ—²ğ˜€ğ˜ ğ—¦ğ—½ğ—¹ğ—¶ğ˜ & ğ— ğ—¼ğ—±ğ—²ğ—¹ ğ—£ğ—¿ğ—²ğ—½ğ—®ğ—¿ğ—®ğ˜ğ—¶ğ—¼ğ—»

In this phase, the focus was on preparing the final dataset for machine learning models.

ğŸ§© Key Steps:

Completed all preprocessing (cleaning, encoding, scaling)

Used train_test_split() from sklearn.model_selection to divide data into training (80%) and testing (20%) sets

Verified data integrity, class balance, and shape consistency using visualization

ğŸ’¡ Key Learnings:

Proper splitting prevents overfitting and ensures accurate evaluation

Apply transformations only on training data to avoid data leakage

ğŸ“‚ Files Added:

Day04/EDA02.ipynb


- ----

ğ——ğ—®ğ˜† ğŸ² â€” ğ—§ğ˜†ğ—½ğ—²ğ˜€ ğ—¼ğ—³ ğ— ğ—®ğ—°ğ—µğ—¶ğ—»ğ—² ğ—Ÿğ—²ğ—®ğ—¿ğ—»ğ—¶ğ—»ğ—´ ğ— ğ—¼ğ—±ğ—²ğ—¹ğ˜€

ğŸ§© Covered:

Supervised Learning â€“ algorithms trained on labeled data

Unsupervised Learning â€“ algorithms explore unlabeled data

Reinforcement Learning â€“ learning via feedback and reward

ğŸ’¡ Key Learnings:

Each ML type serves a unique goal

Model selection depends on data type and problem statement

Laying this foundation simplifies future model choices



- ----

ğ——ğ—®ğ˜† ğŸ³ â€” ğ—Ÿğ—¶ğ—»ğ—²ğ—®ğ—¿ ğ—¥ğ—²ğ—´ğ—¿ğ—²ğ˜€ğ˜€ğ—¶ğ—¼ğ—» ğ—¶ğ—» ğ—”ğ—°ğ˜ğ—¶ğ—¼ğ—»

In this phase, the focus was on exploring Linear Regression, one of the simplest yet powerful supervised learning algorithms.

ğŸ§© Key Steps:

Split the dataset into training (80%) and testing (20%) sets using train_test_split()

Trained a Linear Regression model with scikit-learn

Evaluated model performance using RÂ² Score and Mean Squared Error (MSE)

Visualized Actual vs Predicted values to see the regression line fit

ğŸ’¡ Key Learnings:

Linear Regression can capture patterns and relationships between variables with minimal code

Visualizing predictions helps understand model performance and data trends

ğŸ“‚ Files Added:

Day07/linearregd1.ipynb

- ----

ğŸ§  Day 8 â€“ Logistic Regression (ML Journey)

Day 8 of ML Journey focused on Logistic Regression for classification.

ğŸ“˜ Key Learnings

Logistic Regression is for classification, not regression.

Uses sigmoid function to map predictions to probabilities.

Handles multi-class via One-vs-Rest (OvR).

Simple, fast, and effective baseline model.

ğŸ§© Notebook Overview

Load Digits Dataset (sklearn.datasets)

Visualize sample digits (0â€“9)

Split data (80/20 train/test)

Train Logistic Regression

Evaluate accuracy & visualize predictions

Build confusion matrix heatmap

ğŸ“Š Results

Accuracy: ~95%

Shows common misclassifications (e.g., 3 vs 8)

ğŸ§  Takeaways

Great for fast, interpretable classification

Foundation for SVMs and Neural Networks

Visualization aids deeper insight than accuracy alone

ğŸ› ï¸ Libraries

scikit-learn, numpy, matplotlib, seaborn


- ----

# ğŸ§  Day 09 â€“ Support Vector Machines (SVM)

This notebook covers the basics of **Support Vector Machines** using the **Iris dataset**.

## ğŸ” What is SVM?
SVM is a classification algorithm that finds the best boundary (hyperplane) that separates different classes with the **maximum margin**.

## ğŸ’¡ Why We Use It
- Works well for both linear and non-linear data  
- Uses kernels to handle complex patterns  
- Robust and performs well on small to medium datasets  

## â­ Key Aspects
- Support Vectors  
- Maximum Margin  
- Kernel Trick (linear, RBF, polynomial)  

## ğŸ“˜ What I Did
- Loaded the **Iris dataset**
- Split into train/test sets  
- Trained SVM models with different kernels  
- Checked accuracy  
- Added a few visualizations of predictions and boundaries  

## ğŸ“ Notebook & Code
ğŸ”— (https://github.com/devdocx123/EDA_01)


- ----
